{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection Model with PCA engineered data\n",
    "\n",
    "Now that I've creatd teraining and test data, I'm ready to ready to define and train a model.\n",
    "\n",
    "My goal in this notebook, will be to train a Linear Learner binary classification model that learns to label an answer file as either plagiarized or not, based on the features provided to the model.\n",
    "\n",
    "This task will be broken down into a few discrete steps:\n",
    "\n",
    "* Upload data to S3.\n",
    "* Define a binary classification model and a training script.\n",
    "* Train a Linear Learner binary classifier model and deploy it.\n",
    "* Evaluate deployed classifier and analyze some questions about this approach.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to S3\n",
    "\n",
    "I have created two files: a `training.csv` and `test.csv` file with the features and class labels for the given corpus of plagiarized/non-plagiarized text data, to wich I applyied a PCA.  \n",
    "\n",
    ">The below cells load in some AWS SageMaker libraries and creates a default bucket. After creating this bucket, I'll upload locally stored data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload training data to S3\n",
    "\n",
    "Specifying the `data_dir` where `train.csv` file is saved. Defining a `prefix` where data will be uploaded in the default S3 bucket. Finally, creating a pointer to training data by calling `sagemaker_session.upload_data` and passing in the required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of directory created to save features data\n",
    "data_dir = \"plagiarism_data_pca\"\n",
    "\n",
    "# seting a prefix, a descriptive name for a directory\n",
    "prefix = \"plagiarism_data_pca\"\n",
    "\n",
    "# upload all data to S3\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modeling\n",
    "\n",
    "Now that I've uploaded my training data, it's time to define and train a model!\n",
    "\n",
    "In this notebook, for this binary classification task, I'll use a SageMaker built-in LinearLearner algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create a Linear Learner Estimator\n",
    "I hereby define a Linear Learner model in order to \n",
    "analyze the PCA preprocessed features from training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_ll_output_key_prefix = \"ll_training_output\"\n",
    "s3_ll_output_location = 's3://{}/{}/{}/{}'.format(bucket, prefix, s3_ll_output_key_prefix, 'll_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create linearlearner image\n",
    "import boto3\n",
    "\n",
    "\n",
    "linear_learner = sagemaker.LinearLearner(\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge',\n",
    "    predictor_type='binary_classifier',\n",
    "    epochs=30,\n",
    "    output_path=s3_ll_output_location,\n",
    "    num_models=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join('plagiarism_data_pca', 'train.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =  pd.read_csv(os.path.join('plagiarism_data_pca', 'test.csv')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_df.values[:, :-1].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.squeeze(train_df.values[:, -1:].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_df.values[:, :-1].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.squeeze(test_df.values[:, -1:].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap data in RecordSet objects\n",
    "train_records = linear_learner.record_set(train_features, train_labels, channel='train')\n",
    "test_records = linear_learner.record_set(test_features, test_labels, channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping data into record sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the estimator\n",
    "\n",
    "Training my estimator on the training data stored in S3. This should create a training job that can be monitored in SageMaker console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train estimator on S3 training data\n",
    "linear_learner.fit(train_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# deploy and create a predictor\n",
    "linear_predictor = linear_learner.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating the model\n",
    "\n",
    "Once model is deployed, we can see how it performs when applied to our test data.\n",
    "\n",
    "The provided cell below, reads in the test data, assuming it is stored locally in `data_dir` and named `test.csv`. The labels and features are extracted from the `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# read in test data, assuming it is stored locally\n",
    "test_data = pd.read_csv(os.path.join(data_dir, \"test.csv\"), header=None, names=None)\n",
    "\n",
    "# labels are in the first column\n",
    "test_y = np.squeeze(test_data.iloc[1:,-1:])\n",
    "test_x = np.squeeze(test_data.iloc[1:,:-1])\n",
    "\n",
    "print(test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use deployed `predictor` to generate predicted, class labels for the test data. Compare those to the *true* labels, `test_y`, and calculate the accuracy as a value between 0 and 1.0 that indicates the fraction of test data that the model classified correctly. [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) may be used for this calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: generate predicted, class labels\n",
    "test_y_preds = linear_predictor.predict(test_features.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to evaluate the endpoint on test data\n",
    "# returns a variety of model metrics\n",
    "def evaluate(predictor, test_features, test_labels, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set given the prediction endpoint.  \n",
    "    Return binary classification metrics.\n",
    "    :param predictor: A prediction endpoint\n",
    "    :param test_features: Test features\n",
    "    :param test_labels: Class labels for test data\n",
    "    :param verbose: If True, prints a table of all performance metrics\n",
    "    :return: A dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We have a lot of test data, so we'll split it into batches of 100\n",
    "    # split the test data set into batches and evaluate using prediction endpoint    \n",
    "    prediction_batches = [predictor.predict(batch) for batch in np.array_split(test_features, 1)]\n",
    "    \n",
    "    # LinearLearner produces a `predicted_label` for each data point in a batch\n",
    "    # get the 'predicted_label' for every point in a batch\n",
    "    test_preds = np.concatenate([np.array([x.label['predicted_label'].float32_tensor.values[0] for x in batch]) \n",
    "                                 for batch in prediction_batches])\n",
    "    \n",
    "    # calculate true positives, false positives, true negatives, false negatives\n",
    "    tp = np.logical_and(test_labels, test_preds).sum()\n",
    "    fp = np.logical_and(1-test_labels, test_preds).sum()\n",
    "    tn = np.logical_and(1-test_labels, 1-test_preds).sum()\n",
    "    fn = np.logical_and(test_labels, 1-test_preds).sum()\n",
    "    \n",
    "    # calculate binary classification metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    \n",
    "    # printing a table of metrics\n",
    "    if verbose:\n",
    "        print(pd.crosstab(test_labels, test_preds, rownames=['actual (row)'], colnames=['prediction (col)']))\n",
    "        print(\"\\n{:<11} {:.3f}\".format('Recall:', recall))\n",
    "        print(\"{:<11} {:.3f}\".format('Precision:', precision))\n",
    "        print(\"{:<11} {:.3f}\".format('Accuracy:', accuracy))\n",
    "        print()\n",
    "        \n",
    "    return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn, \n",
    "            'Precision': precision, 'Recall': recall, 'Accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metrics for simple LinearLearner.\\n')\n",
    "\n",
    "# get metrics for linear predictor\n",
    "metrics = evaluate(linear_predictor, \n",
    "                   test_features.astype('float32'), \n",
    "                   test_labels, \n",
    "                   verbose=True) # verbose means we'll print out the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: How many false positives and false negatives did the model produce, if any? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As printed in the above confusion matrix: 15 true positives, 0 false positive, 0 false negatives and 10 true negatives.\n",
    "\n",
    "My guess: LinearLearner applyied to PCA preprocessed feature was also tuned by training 10 different models in parallel. So, very good performance could be reached on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Clean up Resources\n",
    "\n",
    "After model evaluation completion, it is better to **delete model endpoint**. We can do this with a call to `.delete_endpoint()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and fill in the line below!\n",
    "linear_predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting S3 bucket\n",
    "\n",
    "When *completely* done with training and testing models, it is also possible to delete the entire S3 bucket. If done before  training the model, we have to recreate our S3 bucket and upload training data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('daSci': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2cb5f093882d261ebc053314129f29218a3ad00a0912a636e0049baab9e10f0e"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}