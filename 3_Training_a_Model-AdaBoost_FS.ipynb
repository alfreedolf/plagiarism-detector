{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection Model with AdaBoost\n",
    "\n",
    "Now that I've creatd teraining and test data, I'm ready to ready to define and train a model. My goal in this notebook, will be to train a Linear Learner binary classification model that learns to label an answer file as either plagiarized or not, based on the features provided to the model.\n",
    "\n",
    "This task will be broken down into a few discrete steps:\n",
    "\n",
    "* Upload data to S3.\n",
    "* Define a binary classification model and a training script.\n",
    "* Train a Linear Learner binary classifier model and deploy it.\n",
    "* Evaluate deployed classifier and analyze some questions about this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to S3\n",
    "\n",
    "I have created two files: a `training.csv` and `test.csv` file with the features and class labels for the given corpus of plagiarized/non-plagiarized text data, to wich I applyied a PCA.  \n",
    "\n",
    ">The below cells load in some AWS SageMaker libraries and creates a default bucket. After creating this bucket, I'll upload locally stored data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload training data to S3\n",
    "\n",
    "Specifying the `data_dir` where `train.csv` file is saved. Defining a `prefix` where data will be uploaded in the default S3 bucket. Finally, creating a pointer to training data by calling `sagemaker_session.upload_data` and passing in the required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of directory created to save features data\n",
    "data_dir = \"plagiarism_data\"\n",
    "\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = \"plagiarism_data\"\n",
    "\n",
    "# upload all data to S3\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cell\n",
    "\n",
    "Test that your data has been successfully uploaded. The below cell prints out the items in your S3 bucket and will throw an error if it is empty. You should see the contents of your `data_dir` and perhaps some checkpoints. If you see any other files listed, then you may have some old model files that you can delete via the S3 console (though, additional files shouldn't affect the performance of model developed in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# confirm that data is in S3 bucket\n",
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modeling\n",
    "\n",
    "Now that I've uploaded my training data, it's time to define and train a model!\n",
    "\n",
    "In this notebook, for this binary classification task, I'll use a SageMaker AdaBoost ensamble learning meta-algorithm from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory can be changed to: source_sklearn or source_pytorch\n",
    "!pygmentize source_sklearn/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provided code\n",
    "\n",
    "The train.py code includes a few things:\n",
    "* Model loading (`model_fn`) and saving code\n",
    "* Getting SageMaker's default hyperparameters\n",
    "* Loading the training data by name, `train.csv` and extracting the features and labels, `train_x`, and `train_y`\n",
    "\n",
    "To read more about model saving with [joblib for sklearn](https://scikit-learn.org/stable/modules/model_persistence.html) or with [torch.save](https://pytorch.org/tutorials/beginner/saving_loading_models.html), click on the provided links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create an Estimator\n",
    "\n",
    "When a custom model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained; the `train.py` function you specified above. To run a custom training script in SageMaker, construct an estimator, and fill in the appropriate constructor arguments:\n",
    "\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "* **source_dir**: The path to the training script directory `source_sklearn` OR `source_pytorch`.\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "* **source_dir**: The path to the training script directory `train_sklearn` OR `train_pytorch`.\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training.\n",
    "* **source_dir**: The path to the training script directory `train_sklearn` OR `train_pytorch`.\n",
    "* **role**: Role ARN, which was specified, above.\n",
    "* **train_instance_count**: The number of training instances (should be left at 1).\n",
    "* **train_instance_type**: The type of SageMaker instance for training. Note: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* **sagemaker_session**: The session used to train on Sagemaker.\n",
    "* **hyperparameters** (optional): A dictionary `{'name':value, ..}` passed to the train function as hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "# your import and estimator code, here\n",
    "estimator = SKLearn(entry_point='train.py',\n",
    "                    source_dir='source_sklearn',\n",
    "                    train_instance_count = 1,\n",
    "                    train_instance_type='ml.m4.xlarge',\n",
    "                    role=role,\n",
    "                    sagemaker_session = sagemaker_session,\n",
    "                    framework_version='0.20.0',                 \n",
    "                    hyperparameters=\n",
    "                    {\n",
    "                        'learning-rate': 0.005, # learning rate\n",
    "                        'n-estimators': 200,  # num of estimators\n",
    "                    }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the estimator\n",
    "\n",
    "Training the estimator on the training data stored in S3. This should create a training job that can be monitored in SageMaker console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training estimator on S3 training data\n",
    "estimator.fit({'train':input_data})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model\n",
    "\n",
    "After training, we'll deploy the model to create a `predictor`.\n",
    "\n",
    "To deploy a trained model, we'll use `<model>.deploy`, which takes in two arguments:\n",
    "* **initial_instance_count**: The number of deployed instances (1).\n",
    "* **instance_type**: The type of SageMaker instance for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# uncomment, if needed\n",
    "# from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating the Model\n",
    "\n",
    "Once the model is deployed, we can see how it performs when applied to our test data.\n",
    "\n",
    "The provided cell below, reads in the test data, assuming it is stored locally in `data_dir` and named `test.csv`. The labels and features are extracted from the `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# read in test data, assuming it is stored locally\n",
    "test_data = pd.read_csv(os.path.join(data_dir, \"test.csv\"), header=None, names=None)\n",
    "\n",
    "# labels are in the first column\n",
    "test_y = test_data.iloc[:,0]\n",
    "test_x = test_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the accuracy of the model\n",
    "\n",
    "Here we'll use the deployed `predictor` to generate predicted, class labels for the test data. Then we'll compare those to the *true* labels, `test_y`, and calculate the accuracy as a value between 0 and 1.0 that indicates the fraction of test data that the model classified correctly.[sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) may be used for this calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: generate predicted, class labels\n",
    "test_y_preds = predictor.predict(test_x)\n",
    "\n",
    "\n",
    "# test that the model generates the correct number of labels\n",
    "assert len(test_y_preds)==len(test_y), 'Unexpected number of predictions.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# printing accuracy score\n",
    "print(\"accuracy score = {}\".format(accuracy_score(test_y.values, test_y_preds)))\n",
    "\n",
    "# using matplotlib to show confusion matrix\n",
    "fig, ax = plt.subplots(1,1,figsize=(7,4))\n",
    "\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(test_y_preds,test_y,labels=[1,0]),\n",
    "                       display_labels=['Plagiarized', 'Non Plagiarized']).plot(values_format=\".0f\",ax=ax)\n",
    "plt.show()\n",
    "ax.set_xlabel(\"True Label\")\n",
    "ax.set_ylabel(\"Predicted Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: How many false positives and false negatives did hte model produce, if any? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As printed in the above confusion matrix: 15 true positives, 1 false positive, 0 false negatives and 9 true negatives.\n",
    "\n",
    "My guess: AdaBoost is known to be sensitive to noise and outliers. I presume that the misclassied example is an outlier, maybe it is a text that uses a lot of terms that are also used in the reference task text, so it can have a high 1-grams containment value, without being a plagiarized text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Which are the highlights of this kind of model and why we can pick this one? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoostClassifier from sklearn with default DecistionTreeClassifier as base estimator, it's known to be a good classifier on complex classification tasks and it is known not to be overfitting-prone as other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Clean up Resources\n",
    "\n",
    "After done evaluating the model, **delete model endpoint**. We can do this with a call to `.delete_endpoint()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9dbbcf45efe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting S3 bucket\n",
    "\n",
    "When *completely* done with training and testing models, it is also possible to delete the entire S3 bucket. If done before  training the model, we have to recreate our S3 bucket and upload training data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting bucket, uncomment lines below\n",
    "\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('daSci': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2cb5f093882d261ebc053314129f29218a3ad00a0912a636e0049baab9e10f0e"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}